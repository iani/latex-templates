\section{Research Area: Live Coding and Motion Capture in Embodied Dance and Music Performance}
\label{sec:org38e8242}
The proposed project uses low-cost and open source digital technologies to develop new performance techniques based on the feedback between dance and live sound and graphics synthesis.  Indicative of the importance of such digital technologies for performance is the publication of three Oxford Handbook volumes in the past three years dealing with this topic.  The first two volumes (Lewis and Piekut 2016, Piekut and Lewis 2016) deal with Improvisation Studies in the context of technology and culture.  George E. Lewis, one of the two editors and the first editor of the first volume is a pioneer developer of digital music improvisation systems (see Lewis 2007a, 2007b, 2007c, 2017, 2018).  The third Handbook (McLean 2018) deals with Algorithmic Music, and contains extensive contributions on \emph{live coding}, i.e. the practice in which the performer writes the program producing the sound or graphics during the performance itself.  In parallel to the above, technologies of motion capture are increasingly used in dance, theater, entertainment, interactive installations and games.  However the high cost of motion capture systems inhibits the integration of technology in dance due to their limited availability.  As a result of the limited access of dancers to motion capture technology, both their technical know-how and their aesthetic familiarity with this technology is much more limited than that of their fellow performers in experimental, avantgarde, or popular music.  This is a significant cultural disadvantage, given the importance of participation of the human body in the cognitive processes involved in improvisation, action, understanding and music cognition (Iyer 2016: 76ff).  Recently this issue has given rise to the topic of embodied live-coding performance in interactive music performance research (Salazar and Armitage 2018). 

In addition to the aspect of embodiment, motion capture also plays a crucial role in the formation of scores and notation systems as mediator to the enactment of dance.  A renewal of interest in dance and writing as a research tool and as a form of "publishing of choreographic ideas" (deLahunda 2015) is evident in recent years. This marks a radical change in the concept of choreography. The current notion of expanded choreography emphasises choreography as a set of organizational or structural modes on a general level beyond the notion of dance, since "choreography and dancing are two distinct and very different practices" (Forsythe 2008). This approach links choreographic ideas and processes and their mediation through and in relation to other practices (Spier, 2011).  Within this discourse the capture and reproduction of dance movement in a notational or digital system, and its re-enactment as performance by another performer "represents, in a way, the piece itself, separate from the personality or desires of the performer" (Burrows 2010: 142). Merce Cunningham’s use of Lifeforms software in the late 1980ies opened the door to technology as a generator of movement possibilities and marked the beginning of an era.  Technology, neuroscience, cognitive sciences, artificial intelligence and motion capture are only a few fields with which choreography has complementarities in projects by artists such as William Forsythe, Wayne McGregor, Siobhan Davies among others (deLahunta and Zuniga-Shaw 2008). The GiMiC project positions itself within this area of experiments, and investigates means of creating the score/notation live on stage by capturing the movement through sensors in real time. This condition creates an invitation and space for the audience to en ter the work in multiple ways.  The notions of authorship or the choreographer’s authority are put into question, bringing into play an array of collaborative potentials among performers, audience members, musicians and engineers. The practice of performing-capturing-reenacting loops into an autopoietic process which at the same time opens to multiple agents to contribute and to form a non-hierarchical network.

\section{Problems addressed}
\label{sec:org6cc2b39}
The project addresses fundamental problems needed to facilitate research and creation in the field of embodied performance with digital technologies, at the technical, artistic and theoretical level.  Particular technical and cultural challenges currently facing the field and the project are: 

\begin{enumerate}
\item Low-cost solutions for motion capture are not yet widely available. GiMiC uses cutting edge low cost and open source hardware and software technologies.  Some of these are very recent and their evaluation is still underway.  For this, we adopt a bottom-up constructive experimental approach, namely starting with small and achievable challenges and building up to more difficult tasks through several small steps.  We build on the long experience of our host institutions advisory board member in live coding and SuperCollider (Zannos 2011, Zannos and Carlé 2018, Zannos 2019) and also cultivate familiarity with related developments elsewhere in the field (Magnusson 2019, McLean 2018) in order to build a sufficient palette of tools to address the tasks at hand.  Furthermore, we use the exprerience of more advanced teams in low cost motion capture systems (Chordata.cc).
\item Gesture recognition and classification in real time is a cutting edge field that requires both large amounts of data and advanced machine learning techniques to achieve reliable results.  We have started research using basic learning algorithms and statistic techniques and are developing familiarity in their use with motion capture data.
\item Due to the limited accessibility of such technologies to dancers up until very recently, dancers are not familiar with the interactive and embodied approach to dance as performance.  Consequently, there is need to develop both the conceptual and the experiential aspects in this field.  GiMiC addresses this problem through the intense collaboration of a dancer, a musician/live-coder and a dance theorist/choreography specialist.  The collaboration will be documented with text, audiovisual recordings, data, and code in order to produce a repository for future research, and formulate suggestions which can prove useful for futher research.
\end{enumerate}

Through a combined approach on the above challenges, GiMiC aims at producing a viable framework for research and creation in the field, which can be used in the collaborating institutions and shared globally, and furthermore to provide valuable data for research on the role of gestures in embodied performance of combined music and dance with the use of motion capture devices.

\section{Objectives}
\label{sec:org9dc3b25}
From the point of view of performance practice, the overall objective is to grant new ways for expression and room for creativity for composers (who tend to lose ground on automatisation) to programmers (who aim for creativity) the audience (providing them with an alphabet- a chart to follow and understand therefore embody) and performers (providing wider space to interact).  This requires to achieve the following subgoals:

\begin{enumerate}
\item Develop and test motion capture tools based on low-cost Inertial Measuring Units (IMUs) coupled to open-source data processing devices.
\item Develop and test software for interaction with the motion capture tools based on open source software with live-coding capabilities.
\item Develop methodologies for creating embodied live performances with the use of the above tools, and provide theoretical underpinning for connecting this to the overall discussion on technology, embodied performance and improvisation theory.
\item Evaluate the results of the above by sharing the techniques with artists in a residency and workshop event, coupled with documentation and evaluation of data collected with human and machine means.
\end{enumerate}

\section{Methodology}
\label{sec:org3643a80}
\subsection{Gesture as fundamental notion in embodied performance}
\label{sec:org2613de0}
Gesture has been subject of extensive research in the field of music performance (Wanderley and Battier 2000, Schneider 2010).  This project uses results from past research and extends these by applying them to the dancer.  In other words, the dancer is seen as music performer.  This paradigm shift requires two measures:

\begin{enumerate}
\item Develop appropriate techniques of control of sound and graphics by adapting open source software and low cost hardware to the requirements of live performance.
\item Work with choreographers, dancers and coders in order to develop an understanding of gesture as a musical instrument or performance medium, and to jointly develop appropriate feedback mechanisms for sound and image.
\end{enumerate}

The notion of gesture serves in this project as means for abstracting units of movement independently of their specific measured duration or physical trajectory.  This opens up new possibilities of approaching both the semiotic and semantic aspects of gesture in dance (interpretation of gesture meaning within a performance) and the technical aspects.  In particular, this approach entails going beyond the one-dimensional trigger/response interpetation of gesture as it is usually adopted in consumer devices. The project therefore views gesture as a researchable object in a differentiated sense, and while not relying on dependable and fast gesture recognition algorithms, still makes use of available machine learning and statistics tools to evaluate the data and extract information useful for performing artists (see steps 3-5 below).

\subsection{Use of low cost hardware and open source software}
\label{sec:org6ed679e}
We base our work on Inertial Measuring Units that combine accellerometer, gyroscope and magnetometer measurements to provide a vector of 9 positional measurements per sample. These are MEMS-type inertial sensors available at costs between 10 and 37 \texteuro{} each.  We have tested data acquisition and wireless transmission with these using a Raspberry Pi Zero linux computer. The assembly is very small and light and easily wearable.  Using these, we are already able to drive synthesis algorithms in real time on SuperCollider (see Zannos and Carle 2018, Zannos 2019, and recorded rehearals at: \url{https://www.youtube.com/watch?v=xgAcO-Ol21U\&list=PL1yHvCYr9BvY2MUsDpK0dxssgb43WzqDt} and: \url{https://www.youtube.com/watch?v=c2H64zIZqk8}).  Data obtained are stored for further analysis and evaluation, so that gestural characteristics are be obtained to guide choreographic research.

As a next step, we plan to evaluate the prototype of a full body suit that employs 15 sensor units.  This is an open source project by chordata.cc (\url{http://www.chordata.cc}) and has already given convincing results.  It employs hardware very similar to our own smaller prototype and is fully compatible to our setup.  The cost of a suit unit in the current beta-tester program is 400 \texteuro{}. 

For the interconnection to music performance, and the generation of sound and graphics in real time, we use open source or free software including SuperCollider, UnrealEngine, Godot, openFrameworks and others.  The general approach has been described in a recent work by Magnusson (2019), while the technical fundamentals are based on established as well as new libraries and tools for live coding such as JITLib, Tidal Cycles, and sc-hacks (Zannos 2019). 

\subsection{Quantitative analysis of data}
\label{sec:orgd82e0d7}
Data obtained from the motion capture system during rehearsals and performances will be analysed with machine learning techniques starting with basic techniques which have the capacity to work in real time and proceeding to more advanced techniques.  Dr. Agiomyrgianakis has already demonstrated visualisation of Principal Component Analysis from sound data in real time, based on SuperCollider and Python. 

\subsection{Qualitative analysis of experiments}
\label{sec:org1fae270}
Design, development and rehearsal sessions as well as performances will be recorded and documented, in order to provide data for analysis by Drs. Lemi and Tsintziloni from a dance and performance-theoretical point of view.  More data will be provided by recorded interviews with the creative members of the team as well as with audience members and artist participants in the residency and performances organized by the project. 

\subsection{Comparison of quantitative and qualitative results}
\label{sec:org575abd0}
The results of qualitative and quantitative analysis above will be compared in order to find hints about the interpretation of motion capture data as part of the embodied cognitive creative process.

\section{Reseach Environment}
\label{sec:org4afa8c8}
The research hub for the GiMiC team is provided by PEARL (Performative Environments in the Arts Research Laboratory), a research laboratory of the Host Institution (Department of Audiovisual Arts of the Ionian University) founded in 2017.  Since its start, the laboratory has engaged in research on research in performativity with new technologies.  Since 2018 the lab has engaged in research on low cost motion capture devices in dance and live coding, and has already published two papers in the field (Zannos and Carlé 2018, Zannos 2019). The leader of the lab is member of the advisory board of the project and has a more than 20 year old track record on interactive performance and live coding (see Zannos 2011, 2019). 

Garage21 provides the main infastructure for  has an extensive record in organizing residencies, which includes more than 80 participants from 20 different countries in the past 8 years.  In this sense, Garage21 is already an important international hub for contemporary and experimental dance in Greece.  Through its participation in GiMiC it will gain the experience and technical knowhow needed to lead further initiatives in the field for Greece and Southeast EU. 

\section{Deliverables}
\label{sec:org2136995}
\begin{enumerate}
\item A body of recordings of experiments, rehearsals and performances in multiple formats: Audio, Video, Motion Data, Code Libraries, Histories of Time-stamped executed code snippets, Synthesis control data generated during performances.
\item Quantitative evaluation of bodies of data using data
\item Qualitative Evaluation of Methodologies based on questionnaires and recorded interviews with the performers and selected members of the audiences.
\item Publication of at least 2 conference papers (e.g. in ICMC, SMC, NIME or similar) and 2 journal papers.
\item Presentation of performances created by research of the project in international conferences or festivals (ICMC, SMC, ISEA etc.).
\item Organization of a residency, a workshop and at least one public performance event featuring works created by research in this project.
\end{enumerate}

\section{Career Benefits for the participating persons and institutions}
\label{sec:org84ab4b2}
The academic researchers in the team will benefit from the publication of articles in international conferences and in journals.  All members of the team as well as the host and the collaborating institutions will benefit by developing technical expertise in the field and by creating a track record as well as new important connections to institutions and resesarchers engaged in this field.  Furthermore, this project defines itself as one part in the long-term initiative to create technical and cultural infrastructures in Greece for research and creation with affordable digital technologies in the performance arts.  This is done through development of technical and theoretical tools and know-how, which is the focus of this project.

\section{References}
\label{sec:orgea22110}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{0.1cm plus 4mm minus 3mm}
\noindent

Bloom L. A. and Chaplin L. T. 1982. \emph{The Intimate Act of Choreography}. University of Pittsburg Press 1982. 

Burrows, Jonathan. 2010. \emph{A Choreographer's Handbook}. New York : Routledge

deLahunta S. and Zuniga Shaw, N. 2008. "Choreographic Resources Agents, Archives, Scores and Installations, Performance Research". \emph{A Journal of the Performing Arts}, 13:1, 131-133

deLahunta, S. 2015. “Publishing Choreographic Ideas: Discourse from Practice.” Research
Seminar, University of Roehampton, Centre for Dance Research, January 14, 2015. \url{https://}
www.youtube.com/watch?v=x9o8nY8mMyU. (Accessed on March 1, 2019).

Forsythe, W. 2008. Choreographic Objects. Available at: \url{http://www.williamforsythe.de/essay.html} (Last accessed on: March 1, 2019). 

Iyer, V. 2016.  "Improvisation, Action, Understanding, and Music Cognition With and Without Bodies. In:  \emph{The Oxford Handbook of Critical Improvisation Studies, Volume 1}, pp. 74-90. Oxford University Press.

Lewis, E. G. and Piekut, B. (eds.). 2016. \emph{The Oxford Handbook of Critical Improvisation Studies, Volume 1}. Oxford University Press.

Lewis, G. E. 2007. "Live Algorithms and the Future of Music."  \emph{CT Watch Quarterly}, May 2007. \url{http://www.ctwatch.org/quarterly/} (Accessed on March 1, 2019)

Lewis, G. E. 2017. “From Network Bands to Ubiquitous Computing:  Rich Gold and the Social Aesthetics of Interactivity.”  In \emph{Improvisation and Social Aesthetics}, edited by Georgina Born, Eric Lewis, and Will Straw, 91-109. Durham and London: Duke University Press.

Lewis, G. E. 2018. "Why Do We Want Our Computers To Improvise?" In: \emph{The Oxford Handbook of Algorithmic Music}, edited by Alex McLean and Roger T. Dean, 123-30. New York: Oxford University Press.

Lewis, G. E., "Living with Creative Machines: An Improvisor Reflects." In: Anna Everett and Amber J. Wallace, eds. \emph{AfroGEEKS: Beyond the Digital Divide}. Santa Barbara: Center for Black Studies Research, 2007, 83-99.

Lewis, G. E., "Mobilitas Animi: Improvising Technologies, Intending Chance."  \emph{Parallax}, Vol. 13, No. 4, (2007), 108–122.

Magnusson Th. Sonic Music: \emph{Technologies of Material, Symbolic and Signal Inscriptions}, Bloomsbury Academic 2019. 

McLean, A. and Dean, R. (eds.). 2018. \emph{The Oxford Handbook of Algorithmic Music}. New York: Oxford University Press.

Piekut, B. and Lewis, G. E. (eds.). 2016. \emph{The Oxford Handbook of Critical Improvisation Studies, Volume 2}. Oxford University Press. 

Salazar, S. and Armitage, J. 2018. Re-engaging the Body and Gesture in Musical Live Coding. Workshop Paper, International Conference on New Instruments for Musical Expression 2018, (NIME2018). Published online: \url{https://embodiedlivecoding.github.io/nime2018-workshop/workshop-paper.html} (Accessed on March 1, 2019).

Schneider A. 2010. \emph{Music and Gestures: A Historical Introduction and Survey of Earlier Research}. Taylor and Francis. 

Spier S. (ed.) \emph{William Forsythe and the Practice of Choreography: It Starts from Any Point}. Routledge. 2011. 

Wanderley M.M. and Battier M. (eds.). 2000. Trends in Gestural Control in Music, IRCAM-Centre Pompidou.  

Zannos, I. 2011. "Programming in SuperCollider". In: Wilson, S. et al. (eds.): \emph{The SuperCollider Book}. pp. 127-178. MIT Press.

Zannos, I. and Carlé, M. "Metric Interweaving in Networked Dance and Music Performance." In: \emph{Proceedings of the Sound and Music Computing Conference 2018} (SMC18). pp. 524-529. Limassol. 

Zannos, I. 2019. "sc-hacks: A Live Coding Framework for Gestural Performance and Electronic Music" in: \emph{Proceedings of the Linux Audio Conference 2019.} (LAC19) (in print, see \url{http://lac.linuxaudio.org/2019/}). CCRMA, Stanford, Palo Alto.
